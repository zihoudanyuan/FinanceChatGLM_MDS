import argparse
import json
from method.template_manager import template_manager
from method.prompt_generation_v4 import (
    get_balance_static, get_balance_sheet_prompt, get_profit_statement_prompt,
    get_cash_flow_statement_prompt, calculate_indicator, GLMPrompt
)
from modelscope.utils.constant import Tasks
from modelscope import Model
from modelscope.pipelines import pipeline
from tqdm import tqdm
import pandas as pd
import pickle
import os
from collections import defaultdict
import numpy as np
import faiss
import gc
import sys
from io import StringIO
import contextlib
import re
import timeout_decorator


@contextlib.contextmanager
def stdoutIO(stdout=None):
    old = sys.stdout
    if stdout is None:
        stdout = StringIO()
    sys.stdout = stdout
    yield stdout
    sys.stdout = old


model = Model.from_pretrained('/root/autodl-tmp/ZhipuAI/chatglm2-6b', device_map='auto')
pipe = pipeline(task=Tasks.chat, model=model)
COMPUTE_INDEX_SET = [
    'éæµåŠ¨è´Ÿå€ºæ¯”ç‡', 'èµ„äº§è´Ÿå€ºæ¯”ç‡', 'è¥ä¸šåˆ©æ¶¦ç‡', 'é€ŸåŠ¨æ¯”ç‡', 'æµåŠ¨æ¯”ç‡', 'ç°é‡‘æ¯”ç‡', 'å‡€åˆ©æ¶¦ç‡',
    'æ¯›åˆ©ç‡', 'è´¢åŠ¡è´¹ç”¨ç‡', 'è¥ä¸šæˆæœ¬ç‡', 'ç®¡ç†è´¹ç”¨ç‡', "ä¼ä¸šç ”å‘ç»è´¹å è´¹ç”¨",
    'æŠ•èµ„æ”¶ç›Šå è¥ä¸šæ”¶å…¥æ¯”ç‡', 'ç ”å‘ç»è´¹ä¸åˆ©æ¶¦æ¯”å€¼', 'ä¸‰è´¹æ¯”é‡', 'ç ”å‘ç»è´¹ä¸è¥ä¸šæ”¶å…¥æ¯”å€¼', 'æµåŠ¨è´Ÿå€ºæ¯”ç‡'
]

OTHER_TYPE = 'å…¶ä»–'

def get_company_info():
    company_df = pd.read_csv('/root/autodl-tmp/chatglm_llm_fintech_raw_dataset/train.csv')
    print('å¹´æŠ¥æ€»æ•°ï¼š', company_df.shape[0])
    simple = [x.split('__')[3] for x in company_df['name'].tolist()]
    complete = [x.split('__')[1] for x in company_df['name'].tolist()]
    company_info = []
    for name in company_df['name'].tolist():
        parts = name.split('__')
        company_info.append({'date': parts[0],'company':parts[1],'code':parts[2],'simple':parts[3],'report_belong_year':parts[4],'report_type':parts[5].rstrip('.pdf')})
    company_info_df = pd.DataFrame(company_info)
    print(company_info_df.head(2))
    return company_info_df


def cls_question_intention(text, company_list, years=['2019','2020','2021']): # '2019å¹´', '2020å¹´', '2021å¹´'
    """
    """
    time_intention = []
    company_intention = []
    for year in years:
        if year in text:
            time_intention.append(year.strip('å¹´'))
    if len(time_intention) == 0:
        time_intention.append(OTHER_TYPE)
    for company_info in company_list:
        if company_info[0] in text: # or company_info[1] in text
            company_intention.append(company_info[0])
            break
    company_intention = list(set(company_intention))
    if len(company_intention) == 0:
        for company_info in company_list:
            if company_info[1] in text:  # æœ‰badcase
                company_intention.append(company_info[0])
                break
    if len(company_intention) == 0:
        company_intention.append(OTHER_TYPE)
    
    return time_intention, list(set(company_intention))


def read_questions(path):
    with open(path, encoding="utf-8") as file:
        return [json.loads(line) for line in file.readlines()]
    

# @timeout_decorator.timeout(5)
# def exec_timeout(response_):
#     exec(re.findall('```python([.\s\S]*?)```',response_,re.M)[0].strip())


def process_question(question_obj,submit_file='./submit_example.json',knowledge=None,submit_file_detail='./submit_example_detail.json'):
    glm_prompt = GLMPrompt()

    q = question_obj['question']
    # print('here')
    # print(q)
    contains_year, year_ = glm_prompt.find_years(q)
    stock_name, stock_info, has_stock = glm_prompt.has_stock(q)
    compute_index = False

    if contains_year and has_stock:
        for t in COMPUTE_INDEX_SET:
            if t in q:
                prompt_res = calculate_indicator(year_[0], stock_name, index_name=t)
                if prompt_res is not None:
                    prompt_ = template_manager.get_template("ratio_input").format(context=prompt_res, question=q)
                    inputs_t = {'text': prompt_, 'history': []}
                    response_ = pipe(inputs_t)['response']
                    question_obj["answer"] = str(response_)
                    compute_index = True
                    break

    if not compute_index and 'å¢é•¿ç‡' in q:
        statements = [
            get_profit_statement_prompt(q, stock_name, year_),
            get_balance_sheet_prompt(q, stock_name, year_),
            get_cash_flow_statement_prompt(q, stock_name, year_),
            get_balance_static(q, stock_name, year_)
        ]
        prompt_res = [stmt for stmt in statements if len(stmt) > 5]
        if prompt_res:
            prompt_ = template_manager.get_template("ratio_input").format(context=prompt_res, question=q)
            inputs_t = {'text': prompt_, 'history': []}
            response_ = pipe(inputs_t)['response']
            question_obj["answer"] = str(response_)
            compute_index = True

    if not compute_index:
        prompt_ = glm_prompt.handler_q(question_obj['question'],knowledge,pipe)
        if type(prompt_) is list:
            max_python_run_num = 5
            if 'ç ”å‘äººå‘˜ä¸º' in prompt_[0]:
                max_python_run_num = 10
            run_status = False
            for i in range(max_python_run_num):
                try:
                    inputs_t = {'text': prompt_[0], 'history': []} # ç”Ÿæˆpythonä»£ç 
                    response_ = pipe(inputs_t, temperature=0.7)['response']
                    wrong_status = 0
                    # print('############'*2)
                    # print(prompt_[0])
                    # print(response_)
                    # print('############'*3)
                    with stdoutIO() as exec_out:
                        try:
                            # print(response_['response'])
                            # exec(re.findall('```python([.\s\S]*?)```',response_,re.M)[0].strip())
                            if "while True" in response_ or "input" in response_:
                                wrong_status = 1
                                continue
                            else:
                                exec(re.findall('```python([.\s\S]*?)```',response_,re.M)[0].strip())
                            # exec_timeout(response_)
                        except Exception as e1:
                            wrong_status = 1
                            print(str(e1))
                            print("out:", exec_out.getvalue())
                    if wrong_status == 0:
                        question_obj["answer"] = exec_out.getvalue()
                        if '%' in question_obj["answer"]:
                            question_obj["answer"] = question_obj["answer"].replace('%','')
                        run_status = True
                        prompt_ = prompt_[0]
                        # print(exec_out.getvalue())
                        # print('############'*2)
                        break
                except Exception as e:
                    print(e)
            if not run_status:
                # print(f"python calculate wrong:{prompt_[0]}")
                prompt_ = prompt_[1]
                inputs_t = {'text': prompt_[1], 'history': []}
                response_ = pipe(inputs_t)['response']
                question_obj["answer"] = str(response_)
        else:
            # print(prompt_)
            inputs_t = {'text': prompt_, 'history': []}
            response_ = pipe(inputs_t)['response']
            question_obj["answer"] = str(response_)

    with open(submit_file, "a", encoding="utf-8") as f:
        json.dump(question_obj, f, ensure_ascii=False)
        f.write('\n')

    with open(submit_file_detail, "a", encoding="utf-8") as f2:
        question_obj['prompt'] = prompt_
        question_obj['response'] = response_
        json.dump(question_obj, f2, ensure_ascii=False)
        f2.write('\n')

def need_search_in_kg(question_obj):
    glm_prompt = GLMPrompt()

    q = question_obj['question']
    contains_year, year_ = glm_prompt.find_years(q)
    stock_name, stock_info, has_stock = glm_prompt.has_stock(q)
    compute_index = False

    if contains_year and has_stock:
        for t in COMPUTE_INDEX_SET:
            if t in q:
                prompt_res = calculate_indicator(year_[0], stock_name, index_name=t)
                if prompt_res is not None:
                    compute_index = True
                    break

    if not compute_index and 'å¢é•¿ç‡' in q:
        statements = [
            get_profit_statement_prompt(q, stock_name, year_),
            get_balance_sheet_prompt(q, stock_name, year_),
            get_cash_flow_statement_prompt(q, stock_name, year_),
            get_balance_static(q, stock_name, year_)
        ]
        prompt_res = [stmt for stmt in statements if len(stmt) > 5]
        if prompt_res:
            compute_index = True

    if not compute_index:
        return True
    return False

def kg_search(arg):
    # åŠ è½½æµ‹è¯•é›†
    test_data = []
    with open('/root/autodl-tmp/chatglm_llm_fintech_raw_dataset/test_questions.jsonl','rt',encoding='utf-8') as f:
        for line in f:
            question_obj = json.loads(line.strip())
            # if need_search_in_kg(question_obj):
            test_data.append(question_obj)
    # #{'id': 0, 'question': 'èƒ½å¦æ ¹æ®2020å¹´é‡‘å®‡ç”Ÿç‰©æŠ€æœ¯è‚¡ä»½æœ‰é™å…¬å¸çš„å¹´æŠ¥ï¼Œç»™æˆ‘ç®€è¦ä»‹ç»ä¸€ä¸‹æŠ¥å‘ŠæœŸå†…å…¬å¸çš„ç¤¾ä¼šè´£ä»»å·¥ä½œæƒ…å†µï¼Ÿ'}

    test_data = test_data#[:10]

    # æµ‹è¯•é›†é—®é¢˜æ„å›¾è¯†åˆ«
    company_info_df = get_company_info()
    company_list = company_info_df[['company','simple']].to_numpy().tolist()

    test_data_cls = []
    for one_data in test_data:
        question = one_data['question']
        # company_list = get_company_info(question)
        time_intention, company_intention2 = cls_question_intention(question, company_list)
        # if 'åº·å¸Œè¯ºç”Ÿç‰©è‚¡ä»½å…¬å¸' in question:
        #     print(question)
        # if len(company_intention2) > 1:
        #     print(company_intention2)
        assert len(company_intention2) == 1, (company_intention2, question)
        if company_intention2[0] != OTHER_TYPE:
            assert time_intention[0] != OTHER_TYPE

        # time_company_intentions = [time_intention, company_intention]
        # question_intention = company_intention[0]
        test_data_cls.append({'id': one_data['id'], 'question': question, 'company_intention': company_intention2,'time_intention': time_intention})
    test_data_cls.sort(key=lambda x: x['company_intention'][0])
    test_data_group = defaultdict(list)
    for one_data in test_data_cls:
        test_data_group[one_data['company_intention'][0]].append(one_data)
    # return

    # æµ‹è¯•é›†é—®é¢˜embedding
    question_embedding_path = os.path.join(arg.question_embedding_dir, arg.question_embedding_version)
    if not os.path.exists(question_embedding_path):
        os.makedirs(question_embedding_path)
    question_embedding_file = os.path.join(question_embedding_path, arg.question_embedding_file) 
    x = np.array(pickle.load(open(question_embedding_file,'rb')))
    question2embedding = defaultdict()
    for question_info, row_idx in zip(test_data, range(x.shape[0])):
        question2embedding[question_info['id']] = x[row_idx,:].reshape(1,-1)
    
       
    # æœ¬åœ°çŸ¥è¯†åº“ç´¢å¼•åº“åŠ è½½
    index_dir = os.path.join(arg.index_dir, arg.index_version)
    file2index = pickle.load(open(os.path.join(index_dir, 'file2index.pkl'), 'rb'))
    intention2index = defaultdict()
    for file_name, index_file_path in file2index.items():
        time_intention, company_intention3  = cls_question_intention(file_name, company_list)
        # assert len(time_intention) == 1
        assert len(company_intention3) == 1,company_intention3
        intention2index[str(time_intention[0])+'_'+str(company_intention3[0])]=index_file_path[0]
    print("ç´¢å¼•æ–‡ä»¶æ€»æ•°ï¼š", len(intention2index))
    # tmp_cnt = 0
    # for dk,dv in intention2index.items():
    #     print(dk,dv)
    #     tmp_cnt += 1
    #     if tmp_cnt > 100:
    #         break
    # return

    # çŸ¥è¯†åº“æ–‡æœ¬åŠ è½½
    kg_pkl_1 = pickle.load(open(os.path.join('/root/MDS/fintech_data/processed_text/'+arg.knowledge_text_version, 'file2sent_0_6000.pkl'), 'rb'))
    kg_pkl_2 = pickle.load(open(os.path.join('/root/MDS/fintech_data/processed_text/'+arg.knowledge_text_version, 'file2sent_6000_12000.pkl'), 'rb'))
    kg_text_dict = {**kg_pkl_1, **kg_pkl_2}
    intention2kgtext = defaultdict()
    for kg_text_path, kg_sents in kg_text_dict.items():
        time_intention, company_intention  = cls_question_intention(kg_text_path, company_list)
        # assert len(time_intention) == 1
        assert len(company_intention) == 1
        intention2kgtext[str(time_intention[0])+'_'+str(company_intention[0])] = kg_sents
    # return


    # é—®ç­”æ¨¡å‹åŠ è½½
    # model = Model.from_pretrained('/root/autodl-tmp/ZhipuAI/chatglm2-6b', device_map='auto')
    # pipe = pipeline(task=Tasks.chat, model=model)

    # embedding_model = SentenceModel('/root/autodl-tmp/text2vec-large-chinese')
    # file_embedding = model.encode(ret)

    # inputs = {'text':'ä»‹ç»ä¸‹æ¸…åå¤§å­¦', 'history': result['history']}
    # result = pipe(inputs)
    # print(result)
    # {'response': 'æ¸…åå¤§å­¦æ˜¯ä¸­å›½è‘—åçš„é«˜ç­‰æ•™è‚²æœºæ„ä¹‹ä¸€ï¼Œä½äºä¸­å›½åŒ—äº¬å¸‚æµ·æ·€åŒºåŒæ¸…è·¯30å·ï¼Œå…¶å†å²å¯ä»¥è¿½æº¯åˆ° 1911 å¹´åˆ›å»ºçš„æ¸…åå­¦å ‚ï¼Œ1925 å¹´æ”¹åä¸ºæ¸…åå­¦æ ¡ï¼Œæœ€ç»ˆåœ¨ 1949 å¹´æˆä¸ºæ¸…åå¤§å­¦ã€‚ä½œä¸ºæˆ‘å›½é¡¶å°–çš„å¤§å­¦ä¹‹ä¸€ï¼Œæ¸…åå¤§å­¦åœ¨ç§‘å­¦ç ”ç©¶ã€å·¥ç¨‹æŠ€æœ¯ã€ä¿¡æ¯æŠ€æœ¯ã€ç»æµç®¡ç†ç­‰é¢†åŸŸå…·æœ‰æé«˜çš„å£°èª‰å’Œå½±å“åŠ›ã€‚\n\næ¸…åå¤§å­¦çš„æ ¡è®­æ˜¯â€œè‡ªå¼ºä¸æ¯ã€åšå¾·è½½ç‰©â€ï¼Œä½“ç°äº†å­¦æ ¡ä¸€è´¯è¿½æ±‚çš„åŠå­¦ç†å¿µå’Œæ ¸å¿ƒä»·å€¼è§‚ã€‚æ¸…åå¤§å­¦å¼ºè°ƒâ€œä»¥å­¦ç”Ÿä¸ºæœ¬â€ï¼Œæ³¨é‡åˆ›æ–°æ•™è‚²ã€ç´ è´¨æ•™è‚²ï¼Œé¼“åŠ±å­¦ç”Ÿå‹‡äºåˆ›æ–°ã€ç‹¬ç«‹æ€è€ƒï¼ŒåŸ¹å…»å­¦ç”Ÿçš„ç»¼åˆç´ è´¨å’Œå›½é™…åŒ–è§†é‡ã€‚\n\næ¸…åå¤§å­¦æ‹¥æœ‰ä¸€æµçš„å¸ˆèµ„é˜Ÿä¼ï¼ŒåŒ…æ‹¬ä¼—å¤šå›½å†…å¤–çŸ¥åå­¦è€…å’Œä¸“å®¶ï¼Œä¸ºå­¦ç”Ÿæä¾›äº†ä¼˜è´¨çš„æ•™è‚²èµ„æºå’Œè‰¯å¥½çš„å­¦æœ¯æ°›å›´ã€‚æ ¡å›­å†…è®¾æ–½é½å…¨ï¼ŒåŒ…æ‹¬å›¾ä¹¦é¦†ã€å®éªŒå®¤ã€ä½“è‚²é¦†ã€è‰ºæœ¯ä¸­å¿ƒç­‰ï¼Œä¸ºå­¦ç”Ÿæä¾›äº†ä¾¿åˆ©çš„å­¦ä¹ å’Œç”Ÿæ´»æ¡ä»¶ã€‚\n\nè¿‘å¹´æ¥ï¼Œæ¸…åå¤§å­¦åœ¨å›½å†…å¤–å„ä¸ªé¢†åŸŸå–å¾—äº†å·¨å¤§çš„æˆå°±ï¼Œå¦‚åœ¨ç§‘ç ”æ–¹é¢ï¼Œæ¸…åå¤§å­¦çš„ç§‘ç ”å›¢é˜Ÿåœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œå–å¾—äº†ä¸¾ä¸–ç©ç›®çš„æˆæœï¼›åœ¨äººæ‰åŸ¹å…»æ–¹é¢ï¼Œæ¸…åå¤§å­¦åŸ¹å…»äº†å¤§æ‰¹å…·æœ‰å›½é™…è§†é‡å’Œåˆ›æ–°ç²¾ç¥çš„ä¼˜ç§€äººæ‰ï¼Œä¸ºç¤¾ä¼šåšå‡ºäº†é‡è¦è´¡çŒ®ã€‚\n\næ¸…åå¤§å­¦æ˜¯æˆ‘å›½ä¹ƒè‡³å…¨çƒçŸ¥åçš„é«˜ç­‰æ•™è‚²æœºæ„ä¹‹ä¸€ï¼Œä»¥å…¶å“è¶Šçš„å­¦æœ¯æ°´å¹³å’Œä¼˜ç§€çš„äººæ‰åŸ¹å…»æˆæœï¼Œæˆä¸ºäº†å›½å†…å¤–å„ç•Œäººå£«æ‰€èµèª‰çš„é¡¶å°–å­¦åºœã€‚', 'history': [('ä½ å¥½', 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'), ('ä»‹ç»ä¸‹æ¸…åå¤§å­¦', 'æ¸…åå¤§å­¦æ˜¯ä¸­å›½è‘—åçš„é«˜ç­‰æ•™è‚²æœºæ„ä¹‹ä¸€ï¼Œä½äºä¸­å›½åŒ—äº¬å¸‚æµ·æ·€åŒºåŒæ¸…è·¯30å·ï¼Œå…¶å†å²å¯ä»¥è¿½æº¯åˆ° 1911 å¹´åˆ›å»ºçš„æ¸…åå­¦å ‚ï¼Œ1925 å¹´æ”¹åä¸ºæ¸…åå­¦æ ¡ï¼Œæœ€ç»ˆåœ¨ 1949 å¹´æˆä¸ºæ¸…åå¤§å­¦ã€‚ä½œä¸ºæˆ‘å›½é¡¶å°–çš„å¤§å­¦ä¹‹ä¸€ï¼Œæ¸…åå¤§å­¦åœ¨ç§‘å­¦ç ”ç©¶ã€å·¥ç¨‹æŠ€æœ¯ã€ä¿¡æ¯æŠ€æœ¯ã€ç»æµç®¡ç†ç­‰é¢†åŸŸå…·æœ‰æé«˜çš„å£°èª‰å’Œå½±å“åŠ›ã€‚\n\næ¸…åå¤§å­¦çš„æ ¡è®­æ˜¯â€œè‡ªå¼ºä¸æ¯ã€åšå¾·è½½ç‰©â€ï¼Œä½“ç°äº†å­¦æ ¡ä¸€è´¯è¿½æ±‚çš„åŠå­¦ç†å¿µå’Œæ ¸å¿ƒä»·å€¼è§‚ã€‚æ¸…åå¤§å­¦å¼ºè°ƒâ€œä»¥å­¦ç”Ÿä¸ºæœ¬â€ï¼Œæ³¨é‡åˆ›æ–°æ•™è‚²ã€ç´ è´¨æ•™è‚²ï¼Œé¼“åŠ±å­¦ç”Ÿå‹‡äºåˆ›æ–°ã€ç‹¬ç«‹æ€è€ƒï¼ŒåŸ¹å…»å­¦ç”Ÿçš„ç»¼åˆç´ è´¨å’Œå›½é™…åŒ–è§†é‡ã€‚\n\næ¸…åå¤§å­¦æ‹¥æœ‰ä¸€æµçš„å¸ˆèµ„é˜Ÿä¼ï¼ŒåŒ…æ‹¬ä¼—å¤šå›½å†…å¤–çŸ¥åå­¦è€…å’Œä¸“å®¶ï¼Œä¸ºå­¦ç”Ÿæä¾›äº†ä¼˜è´¨çš„æ•™è‚²èµ„æºå’Œè‰¯å¥½çš„å­¦æœ¯æ°›å›´ã€‚æ ¡å›­å†…è®¾æ–½é½å…¨ï¼ŒåŒ…æ‹¬å›¾ä¹¦é¦†ã€å®éªŒå®¤ã€ä½“è‚²é¦†ã€è‰ºæœ¯ä¸­å¿ƒç­‰ï¼Œä¸ºå­¦ç”Ÿæä¾›äº†ä¾¿åˆ©çš„å­¦ä¹ å’Œç”Ÿæ´»æ¡ä»¶ã€‚\n\nè¿‘å¹´æ¥ï¼Œæ¸…åå¤§å­¦åœ¨å›½å†…å¤–å„ä¸ªé¢†åŸŸå–å¾—äº†å·¨å¤§çš„æˆå°±ï¼Œå¦‚åœ¨ç§‘ç ”æ–¹é¢ï¼Œæ¸…åå¤§å­¦çš„ç§‘ç ”å›¢é˜Ÿåœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œå–å¾—äº†ä¸¾ä¸–ç©ç›®çš„æˆæœï¼›åœ¨äººæ‰åŸ¹å…»æ–¹é¢ï¼Œæ¸…åå¤§å­¦åŸ¹å…»äº†å¤§æ‰¹å…·æœ‰å›½é™…è§†é‡å’Œåˆ›æ–°ç²¾ç¥çš„ä¼˜ç§€äººæ‰ï¼Œä¸ºç¤¾ä¼šåšå‡ºäº†é‡è¦è´¡çŒ®ã€‚\n\næ¸…åå¤§å­¦æ˜¯æˆ‘å›½ä¹ƒè‡³å…¨çƒçŸ¥åçš„é«˜ç­‰æ•™è‚²æœºæ„ä¹‹ä¸€ï¼Œä»¥å…¶å“è¶Šçš„å­¦æœ¯æ°´å¹³å’Œä¼˜ç§€çš„äººæ‰åŸ¹å…»æˆæœï¼Œæˆä¸ºäº†å›½å†…å¤–å„ç•Œäººå£«æ‰€èµèª‰çš„é¡¶å°–å­¦åºœã€‚')]}
 

    # æµ‹è¯•é›†ç­”æ¡ˆç”Ÿæˆ
    loaded_indexes = []
    file2loaded_index = defaultdict()
    MAX_LOADED_INDEX_NUM = 100
    test_data_answer = []
    # question_context = defaultdict(list)
    faiss_nprobe = 10
    before_context_num = 2
    after_context_num = 3
    faiss_top_k = 3
    MAX_CONTEXT_LEN = 7200

    # debug_cnt = 0
    miss_cnt = 0

    test_data_knowledge = defaultdict()

    for intention, questions in tqdm(test_data_group.items()): # company_intention
        if intention == OTHER_TYPE:
            miss_cnt += 1
            # continue
            # for question in questions:
            #     model_input = {'text': question['question'], 'history': []}
            #     result = pipe(model_input)
            #     test_data_answer.append({'id': question['id'],'question': question['question'], 'intention': OTHER_TYPE, 'answer': result['response']})
        else:
            # debug_cnt += 1
            # if debug_cnt > 3:
            #     break
            # åŠ è½½ç´¢å¼•åº“
            # {'id': one_data['id'], 'question': question, 'company_intention': company_intention,'time_intention': time_intention}
            for question in questions:
                time_intentions = question['time_intention']
                company_intentions = question['company_intention'][0]
                query_indexes = []
                for ti, ci in zip(time_intentions, [company_intentions]*len(time_intentions)):
                    query_indexes.append(str(ti)+'_'+str(ci))
                question_kg_context = []
                question_kg_context_every_one = []
                for query_index in query_indexes:
                    if query_index not in loaded_indexes:
                        if len(loaded_indexes) >= MAX_LOADED_INDEX_NUM:
                            del file2loaded_index[loaded_indexes[0]]
                            loaded_indexes.pop(0)
                            gc.collect()
                        if query_index not in intention2index:  # æ²¡æœ‰æ‰¾åˆ°æœ¬åœ°ç´¢å¼•æ–‡ä»¶ï¼Œæ”¾å¼ƒæ£€ç´¢
                            query_parts = query_index.split('_')
                            query_year = int(query_parts[0])
                            if str(query_year+1)+'_'+str(query_parts[1]) in intention2index:
                                query_index = str(query_year+1)+'_'+str(query_parts[1])
                            elif str(query_year+2)+'_'+str(query_parts[1]) in intention2index:
                                query_index = str(query_year+2)+'_'+str(query_parts[1])
                            elif str(query_year-1)+'_'+str(query_parts[1]) in intention2index:
                                query_index = str(query_year-1)+'_'+str(query_parts[1])
                            elif str(query_year-2)+'_'+str(query_parts[1]) in intention2index:
                                query_index = str(query_year-2)+'_'+str(query_parts[1])
                            else:
                                print(query_index," not in intention2index !!!")
                                continue
                        if query_index not in loaded_indexes:
                            loaded_indexes.append(query_index)
                        loaded_faiss_index = faiss.read_index(intention2index[query_index])
                        file2loaded_index[query_index] = loaded_faiss_index
                    question_vector = question2embedding[question['id']]
                    faiss_index = file2loaded_index[query_index]
                    faiss_index.nprobe = faiss_nprobe
                    # if 'çˆ±æ—­è‚¡ä»½' in question['question']:
                    #     dummy_q = 'æ²ˆé¸¿çƒˆæ›¾åœ¨å“ªé‡Œä»äº‹åšå£«åç ”ç©¶'
                    #     dummy_q_embedding = embedding_model.encode([dummy_q])
                    #     question_vector = dummy_q_embedding
                    #     print('dummy_qï¼š',dummy_q)
                    _, neighbors = faiss_index.search(question_vector, faiss_top_k)
                    # print(neighbors)
                    if query_index not in intention2kgtext:
                        print(query_index,"not in intention2kgtext")
                        continue

                    neighbor_all_indexes = []
                    for neighbor in neighbors[0]:
                        # print(neighbor,neighbors)
                        neighbor_begin = max(0, neighbor-before_context_num)
                        neighbor_end = min(len(intention2kgtext[query_index]), neighbor+after_context_num)
                        # print(neighbor,neighbor_begin,neighbor_end)
                        for ni in range(neighbor_begin,neighbor_end,1):
                            # print(ni)
                            neighbor_all_indexes.append(ni)
                    # print(neighbor_all_indexes)
                    neighbor_all_indexes = list(set(neighbor_all_indexes))
                    neighbor_all_indexes.sort(reverse=False)
                    # print(neighbor_all_indexes)
                    neighbor_texts = []
                    for neighbor_index in neighbor_all_indexes:
                        # if query_index not in intention2kgtext:
                        #     print(query_index,"not in intention2kgtext")
                        neighbor_texts.append(intention2kgtext[query_index][neighbor_index])
                    # print(neighbor_texts)
                    question_kg_context.extend(neighbor_texts)
                    question_kg_context_every_one.append(neighbor_texts)
                if len(question_kg_context) == 0:
                    continue
                    # print(query_indexes,'find no information in local file')
                    # kg_context = ''
                    # kg_prompt = question['question']
                    # model_input = {'text': kg_prompt, 'history': []}
                    # result = pipe(model_input)
                    # test_data_answer.append({'id': question['id'],'question': question['question'], 'intention': '|'.join(query_indexes), 'answer': result['response'],'final_question': kg_prompt,'kg_context':kg_context,'kg_context_parts':json.dumps(question_kg_context_every_one, ensure_ascii=False)})
                else:
                    # åŸºäºä¸Šä¸‹æ–‡çš„promptæ¨¡ç‰ˆï¼Œè¯·åŠ¡å¿…ä¿ç•™"{question}"å’Œ"{context}"
                    kg_context = '\n'.join(question_kg_context)
                    # kg_question = question['question']
                    # if 'çˆ±æ—­è‚¡ä»½' in question['question']:
                    #     kg_question = 'æ²ˆé¸¿çƒˆæ›¾åœ¨å“ªé‡Œä»äº‹åšå£«åç ”ç©¶'
                    if len(kg_context) > MAX_CONTEXT_LEN:
                        kg_context = kg_context[:MAX_CONTEXT_LEN]
                    # print(kg_question)
                    # print(kg_context)
#                     kg_prompt = """å·²çŸ¥ä¿¡æ¯ï¼š
# {kg_context} 

# æ ¹æ®ä¸Šè¿°å·²çŸ¥ä¿¡æ¯ï¼Œç®€æ´å’Œä¸“ä¸šçš„æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»ä¸­å¾—åˆ°ç­”æ¡ˆï¼Œè¯·è¯´ â€œæ ¹æ®å·²çŸ¥ä¿¡æ¯æ— æ³•å›ç­”è¯¥é—®é¢˜â€ æˆ– â€œæ²¡æœ‰æä¾›è¶³å¤Ÿçš„ç›¸å…³ä¿¡æ¯â€ï¼Œä¸å…è®¸åœ¨ç­”æ¡ˆä¸­æ·»åŠ ç¼–é€ æˆåˆ†ï¼Œç­”æ¡ˆè¯·ä½¿ç”¨ä¸­æ–‡ã€‚ é—®é¢˜æ˜¯ï¼š{kg_question}""".format(kg_context=kg_context,kg_question=kg_question)
#                     model_input = {'text': kg_prompt, 'history': []}
#                     result = pipe(model_input)
#                     test_data_answer.append({'id': question['id'],'question': kg_question, 'intention': '|'.join(query_indexes), 'answer': result['response'],'final_question': kg_prompt,'kg_context':kg_context,'kg_context_parts':json.dumps(question_kg_context_every_one, ensure_ascii=False)})

                    test_data_knowledge[question['id']] = kg_context
    
    # test_data_answer.sort(key=lambda x: x['id'],reverse=False)
    # with open(arg.submit_file,'wt',encoding='utf8') as f:
    #     for one in test_data_answer:
    #         dummy = {'id' : one['id'],'question': one['question'],'answer': one['answer']}
    #         f.write(json.dumps(dummy, ensure_ascii=False)+'\n')
    # pickle.dump(test_data_answer,open(arg.submit_detail_file,'wb'))
    # print('miss_cnt', miss_cnt)
    # print('debug_cnt', debug_cnt)
    return  test_data_knowledge


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Answer question by vector search and semantic search.')
    parser.add_argument("--test_file", type=str, default="/root/autodl-tmp/chatglm_llm_fintech_raw_dataset/test_questions.jsonl")
    parser.add_argument("--index_dir", type=str, default="/root/autodl-tmp/fintech_data/index")
    parser.add_argument("--index_version", type=str, default="v2")
    parser.add_argument("--question_embedding_dir", type=str,
                        help="process annual report begin offset")
    parser.add_argument("--question_embedding_version", type=str,
                        help="process annual report end offset")
    parser.add_argument("--question_embedding_file", type=str,
                        help="process annual report end offset")
    parser.add_argument("--knowledge_text_version", type=str,
                        help="process annual report end offset")
    parser.add_argument("--submit_file", type=str,default='./submit_example.json',
                        help="process annual report end offset")
    parser.add_argument("--submit_detail_file", type=str,
                        help="process annual report end offset")
    parser.add_argument("--question_knowledge_store_file", type=str,
                        help="process annual report end offset")
    arg = parser.parse_args()

    # test_data_knowledge = kg_search(arg)
    # pickle.dump(test_data_knowledge, open(os.path.join(arg.index_dir,arg.index_version+'_'+arg.question_knowledge_store_file), "wb"))
    test_data_knowledge = pickle.load(open(os.path.join(arg.index_dir,arg.index_version+'_'+arg.question_knowledge_store_file), "rb"))
    questions = read_questions("./data/test_questions.jsonl")

    for idx, question_obj in enumerate(tqdm(questions)):
        # if 'ç ”å‘äººå‘˜å èŒå·¥' not in question_obj['question']:
        #     continue
        # if 'ç¡•å£«åŠä»¥ä¸Šäººå‘˜å ' not in question_obj['question']:
        #     continue

        # print(f'Processing question {idx}\n')
        if question_obj['id'] in test_data_knowledge:
            process_question(question_obj,arg.submit_file,knowledge=test_data_knowledge[question_obj['id']],submit_file_detail=arg.submit_detail_file)
        else:
            process_question(question_obj,arg.submit_file,knowledge=None,submit_file_detail=arg.submit_detail_file)

    # python glm_demo_v2.py  --index_dir=/root/autodl-tmp/fintech_data/index --index_version=v3  --question_embedding_dir=/root/autodl-tmp/fintech_data/question_embedding --question_embedding_version=v3 --question_embedding_file=question_embedding_file.pkl  --knowledge_text_version=v3  --question_knowledge_store_file=question_knowledge_store_file.pkl

    # python glm_demo_v3.py  --index_dir=/root/autodl-tmp/fintech_data/index --index_version=v3  --question_embedding_dir=/root/autodl-tmp/fintech_data/question_embedding --question_embedding_version=v3 --question_embedding_file=question_embedding_file.pkl  --knowledge_text_version=v3  --question_knowledge_store_file=question_knowledge_store_file.pkl --submit_file=./æœºå™¨çˆ±å­¦ä¹ _result_0812_v3.json

    # python glm_demo_v3.py  --index_dir=/root/autodl-tmp/fintech_data/index --index_version=v3  --question_embedding_dir=/root/autodl-tmp/fintech_data/question_embedding --question_embedding_version=v3 --question_embedding_file=question_embedding_file.pkl  --knowledge_text_version=v3  --question_knowledge_store_file=question_knowledge_store_file.pkl --submit_file=./æœºå™¨çˆ±å­¦ä¹ _result_0812_v3.json --submit_detail_file=./æœºå™¨çˆ±å­¦ä¹ _result_0812_v3_detail.json